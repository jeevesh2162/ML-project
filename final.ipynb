{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9719ec38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Importing XGBClassifier for the XGBoost model\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "# We now import RandomizedSearchCV instead of GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import RandomizedSearchCV \n",
    "# We need scipy.stats for defining parameter distributions\n",
    "from scipy.stats import uniform, randint\n",
    "import numpy as np\n",
    "\n",
    "# --- Configuration ---\n",
    "# Number of random parameter combinations to test.\n",
    "# This gives better coverage than the fixed grid search.\n",
    "N_ITER_SEARCH = 50 \n",
    "# ---------------------\n",
    "\n",
    "# --- 1. Load Data ---\n",
    "# We use a try/except block to handle file loading errors gracefully.\n",
    "try:\n",
    "    train_df = pd.read_csv('train.csv')\n",
    "    test_df = pd.read_csv('test.csv')\n",
    "    sample_submission_df = pd.read_csv('sample_submission.csv')\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: One or more files not found. Ensure 'train.csv', 'test.csv', and 'sample_submission.csv' are available. {e}\")\n",
    "    exit()\n",
    "\n",
    "print(\"Data loaded successfully.\")\n",
    "\n",
    "# Separate IDs needed for the final submission file\n",
    "test_ids = test_df['id']\n",
    "\n",
    "# Remove the 'id' column from both dataframes as it's not a feature\n",
    "train_df = train_df.drop('id', axis=1)\n",
    "test_df = test_df.drop('id', axis=1) \n",
    "\n",
    "# --- 2. Separate Features (X) and Target (y) ---\n",
    "X = train_df.drop('WeightCategory', axis=1)\n",
    "y = train_df['WeightCategory']\n",
    "\n",
    "# --- 3. Data Cleaning, Imputation, and Feature Engineering (NEW STEP) ---\n",
    "print(\"3. Feature Engineering and Imputation...\")\n",
    "\n",
    "# Identify columns by type\n",
    "numerical_cols_raw = X.select_dtypes(include=np.number).columns\n",
    "categorical_cols_raw = X.select_dtypes(include=['object']).columns\n",
    "\n",
    "# A. Calculate BMI: Weight / (Height^2). This is highly predictive.\n",
    "def calculate_bmi(df):\n",
    "    # Replace zero height with a tiny number (1e-6) to prevent division by zero\n",
    "    df['Height'] = df['Height'].replace(0, 1e-6) \n",
    "    df['BMI'] = df['Weight'] / (df['Height'] ** 2)\n",
    "    return df\n",
    "\n",
    "# Apply BMI calculation to both datasets\n",
    "X = calculate_bmi(X)\n",
    "test_df = calculate_bmi(test_df)\n",
    "print(\"   -> Added highly predictive 'BMI' feature.\")\n",
    "\n",
    "# B. Imputation: Fill missing values\n",
    "# Imputation for Numerical Features: Fill missing values with the median (robust to outliers)\n",
    "for col in numerical_cols_raw:\n",
    "    median_val = X[col].median()\n",
    "    X[col] = X[col].fillna(median_val)\n",
    "    test_df[col] = test_df[col].fillna(median_val)\n",
    "\n",
    "# Imputation for Categorical Features: Fill missing values with 'Missing'\n",
    "for col in categorical_cols_raw:\n",
    "    X[col] = X[col].fillna('Missing')\n",
    "    test_df[col] = test_df[col].fillna('Missing')\n",
    "print(\"   -> Imputed missing values with median/Missing.\")\n",
    "\n",
    "\n",
    "# --- 4. Pre-processing: Handling Categorical Data (OHE) ---\n",
    "categorical_cols = X.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "X_processed = pd.get_dummies(X, columns=categorical_cols, drop_first=True)\n",
    "X_test_processed = pd.get_dummies(test_df, columns=categorical_cols, drop_first=True)\n",
    "\n",
    "# Align columns\n",
    "missing_cols = set(X_processed.columns) - set(X_test_processed.columns)\n",
    "for c in missing_cols:\n",
    "    X_test_processed[c] = 0\n",
    "X_test_processed = X_test_processed[X_processed.columns]\n",
    "\n",
    "\n",
    "# --- 5. Pre-processing: Feature Scaling ---\n",
    "# The 'BMI' feature is automatically included here for scaling.\n",
    "scaler = StandardScaler()\n",
    "numerical_cols = X_processed.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "X_processed[numerical_cols] = scaler.fit_transform(X_processed[numerical_cols])\n",
    "X_test_processed[numerical_cols] = scaler.transform(X_test_processed[numerical_cols])\n",
    "\n",
    "\n",
    "# --- 6. Target Encoding ---\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "target_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
    "print(\"\\n--- Target Class Mapping (for reference) ---\")\n",
    "for category, code in target_mapping.items():\n",
    "    print(f\"Code {code}: {category}\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "\n",
    "# --- 7. Hyperparameter Tuning using RandomizedSearchCV ---\n",
    "\n",
    "print(f\"Starting Randomized Hyperparameter Tuning with {N_ITER_SEARCH} total trials...\")\n",
    "\n",
    "# Define the parameter DISTRIBUTIONS to sample from\n",
    "param_distributions = {\n",
    "    # Number of trees: Test a wider, random range\n",
    "    'n_estimators': randint(100, 700), \n",
    "    # Learning rate: Sample continuously between 0.005 and 0.2\n",
    "    'learning_rate': uniform(0.005, 0.195), \n",
    "    # Tree depth: Sample integers between 3 and 10\n",
    "    'max_depth': randint(3, 11), \n",
    "    # Subsample (row sampling): Sample between 0.5 and 1.0\n",
    "    'subsample': uniform(0.5, 0.5),\n",
    "    # Colsample (feature sampling): Sample between 0.5 and 1.0\n",
    "    'colsample_bytree': uniform(0.5, 0.5), \n",
    "    # L2 regularization: Test a few discrete values\n",
    "    'reg_lambda': [0.1, 1, 10]\n",
    "}\n",
    "\n",
    "# Initialize the base XGBoost model\n",
    "base_model = XGBClassifier(\n",
    "    objective='multi:softmax',\n",
    "    use_label_encoder=False, \n",
    "    eval_metric='mlogloss',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Initialize RandomizedSearchCV\n",
    "# We set n_iter=50 random combinations\n",
    "# cv=3 means 3-fold cross-validation\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=base_model, \n",
    "    param_distributions=param_distributions, \n",
    "    n_iter=N_ITER_SEARCH, # Number of randomized trials\n",
    "    scoring='accuracy', # The metric to optimize\n",
    "    cv=3, \n",
    "    verbose=2, # Increased verbosity to track progress\n",
    "    random_state=42, # Ensure reproducibility of the random sampling\n",
    "    n_jobs=-1 # Use all available cores\n",
    ")\n",
    "\n",
    "# Perform the search\n",
    "random_search.fit(X_processed, y_encoded)\n",
    "\n",
    "# The best model found after tuning\n",
    "model = random_search.best_estimator_\n",
    "\n",
    "print(\"\\nTuning complete.\")\n",
    "print(f\"Total Randomized Trials: {N_ITER_SEARCH}\")\n",
    "print(f\"Best parameters found: {random_search.best_params_}\")\n",
    "\n",
    "\n",
    "# --- 8. Evaluate on Training Data and Generate Predictions for the Test Set ---\n",
    "\n",
    "# Calculate and print the training accuracy using the tuned model\n",
    "y_train_pred_encoded = model.predict(X_processed)\n",
    "train_accuracy = accuracy_score(y_encoded, y_train_pred_encoded)\n",
    "print(f\"Tuned Model Training Accuracy: {train_accuracy:.4f}\")\n",
    "\n",
    "print(\"Generating predictions on the test set...\")\n",
    "y_pred_encoded = model.predict(X_test_processed)\n",
    "\n",
    "# Convert the numerical predictions back into the original category names\n",
    "y_pred_categories = label_encoder.inverse_transform(y_pred_encoded)\n",
    "\n",
    "\n",
    "# --- 9. Create Submission File ---\n",
    "\n",
    "submission_df = pd.DataFrame({\n",
    "    'id': test_ids,\n",
    "    'WeightCategory': y_pred_categories\n",
    "})\n",
    "\n",
    "# Save the final submission file\n",
    "submission_filepath = 'submission_xgboost_random_search_bmi.csv' # Updated filename\n",
    "submission_df.to_csv(submission_filepath, index=False)\n",
    "\n",
    "print(f\"\\nSuccessfully generated predictions and saved to: {submission_filepath}\")\n",
    "print(\"\\n--- Submission Head (First 5 Rows) ---\")\n",
    "print(submission_df.head().to_string(index=False))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
